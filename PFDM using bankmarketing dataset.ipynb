{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expensive-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaray\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from eval_metrics_cls import *\n",
    "import random\n",
    "import math\n",
    "import copy, time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defined-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seting hyperparameters:\n",
    "inner_lr = 0.01  # learning rate of the inner loop\n",
    "dual_ud_lr = 0.01\n",
    "meta_lr = 0.001  # learning rate of the outer loop\n",
    "meta_dual_lr = 0.01\n",
    "\n",
    "lamb = 1\n",
    "K = 10  # K-shot per task for support data\n",
    "Kq = K * 2  # Kq-shot per task for query data\n",
    "c = 0.05  # fairness bound\n",
    "inner_steps = 1  # number of gradient steps in the inner loop, normally equals to 1 in MAML\n",
    "pd_updates = 10\n",
    "tasks_per_meta_batch = 8  # number of tasks sampled from tasks repository\n",
    "num_iterations = 30  # number of iterations of the outer loop\n",
    "\n",
    "# other parameters\n",
    "num_feature = 16  # number of features of the input data for each task\n",
    "num_neighbors = 3\n",
    "plot_every = 3\n",
    "print_every = 25\n",
    "repeat = 2\n",
    "\n",
    "# PATHs\n",
    "task_path = '/home/mifeng/Data/'+'bankmarketing/'\n",
    "train_tasks_path = task_path+r'train'\n",
    "train_model_save_path = r'train_model.pth'\n",
    "tr_val_meta_losses_faires_save_path = r'tr_val_meta_losses_faires_save.txt'\n",
    "\n",
    "test_tasks_path = task_path+r'test'\n",
    "val_tasks_path = task_path+r'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dynamic-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for evaluation metrics:\n",
    "def cal_discrimination(input_zy):\n",
    "    a_values = []\n",
    "    b_values = []\n",
    "\n",
    "    for line in input_zy:\n",
    "        if line[0] == 0:\n",
    "            a_values.append(line[1])\n",
    "        elif line[0] == 1:\n",
    "            b_values.append(line[1])\n",
    "\n",
    "    if len(a_values) == 0:\n",
    "        discrimination = sum(b_values) * 1.0 / len(b_values)\n",
    "    elif len(b_values) == 0:\n",
    "        discrimination = sum(a_values) * 1.0 / len(a_values)\n",
    "    else:\n",
    "        discrimination = sum(a_values) * 1.0 / len(a_values) - sum(b_values) * 1.0 / len(b_values)\n",
    "    return abs(discrimination)\n",
    "\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1) - 1):\n",
    "        distance += (row1[i] - row2[i]) ** 2\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "# example: get_neighbors(yX, X[0], 3)\n",
    "def get_neighbors(yX, target_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for yX_row in yX:\n",
    "        X_row = yX_row[1:]\n",
    "        y = yX_row[0]\n",
    "        dist = euclidean_distance(target_row, X_row)\n",
    "        distances.append((y, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def cal_consistency(yX, num_neighbors):\n",
    "    ans = 0\n",
    "    for yX_row in yX:\n",
    "        temp = 0\n",
    "        target_row = yX_row[1:]\n",
    "        target_y = yX_row[0]\n",
    "        y_neighbors = get_neighbors(yX, target_row, num_neighbors)\n",
    "        for y_neighbor in y_neighbors:\n",
    "            temp += abs(target_y - y_neighbor)\n",
    "        ans += temp\n",
    "    return (1 - (ans * 1.0) / (len(yX) * num_neighbors))\n",
    "\n",
    "\n",
    "def cal_dbc(input_zy):\n",
    "    length = len(input_zy)\n",
    "    z_bar = np.mean(input_zy[:, 0])\n",
    "    dbc = 0\n",
    "    for zy in input_zy:\n",
    "        dbc += (zy[0] - z_bar) * zy[1] * 1.0\n",
    "    return abs(dbc / length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "invisible-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for maml model:\n",
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(16, 40)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('l2', nn.Linear(40, 40)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('l3', nn.Linear(40, 1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def parameterised(self, x, weights):\n",
    "        # like forward, but uses ``weights`` instead of ``model.parameters()``\n",
    "        # it'd be nice if this could be generated automatically for any nn.Module...\n",
    "        # print(\"X shape\",x.shape)\n",
    "        # print(\"weights[0].shape\",weights[0])\n",
    "        # print(\"weights[1].shape\",weights[1])\n",
    "        x = nn.functional.linear(x, weights[0], weights[1])\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.linear(x, weights[2], weights[3])\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.linear(x, weights[4], weights[5])\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class MAML():\n",
    "    \"\"\"\n",
    "    This code implements MAML for supervised few-shot regression learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tasks, num_feature, lamb, inner_lr, dual_ud_lr, meta_lr, meta_dual_lr, K, Kq, inner_steps, pd_updates, tasks_per_meta_batch, c, plot_every, print_every, val_tasks_path):\n",
    "        \"\"\"\n",
    "            tasks: name collection of each task -- list of strings\n",
    "            task: df -- pandas data frame\n",
    "        \"\"\"\n",
    "\n",
    "        # important objects\n",
    "        self.tasks = tasks\n",
    "        self.model = model\n",
    "        self.num_feature = num_feature\n",
    "        self.weights = list(model.parameters())  # the maml weights (primal-variable) we will be meta-optimising\n",
    "        self.lamb = lamb  # the dual-variable we will be meta-optimising\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.meta_optimiser = torch.optim.Adam(self.weights, meta_lr)\n",
    "\n",
    "        # hyperparameters\n",
    "        self.inner_lr = inner_lr\n",
    "        self.dual_ud_lr = dual_ud_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.meta_dual_lr = meta_dual_lr\n",
    "        self.K = K\n",
    "        self.Kq = Kq\n",
    "        self.inner_steps = inner_steps  # with the current design of MAML, >1 is unlikely to work well\n",
    "        self.pd_updates = pd_updates\n",
    "        self.tasks_per_meta_batch = tasks_per_meta_batch\n",
    "        self.c = c\n",
    "\n",
    "        # metrics\n",
    "        self.plot_every = plot_every\n",
    "        self.print_every = print_every\n",
    "\n",
    "        self.meta_losses = []\n",
    "        self.meta_faires = []\n",
    "        self.meta_accus = []\n",
    "\n",
    "        self.train_losses_list = []\n",
    "        self.train_faires_list = []\n",
    "        self.train_accus_list = []\n",
    "\n",
    "        self.val_losses_list = []\n",
    "        self.val_faires_list = []\n",
    "        self.val_accus_list = []\n",
    "\n",
    "        # others\n",
    "        self.val_tasks_path = val_tasks_path\n",
    "\n",
    "    def mean(self, a):\n",
    "        return sum(a).to(dtype=torch.float) / len(a)\n",
    "\n",
    "    def inner_loop(self, task):\n",
    "        # reset inner model to current maml weights\n",
    "        temp_weights = [w.clone() for w in self.weights]\n",
    "        try:\n",
    "            temp_lambda = copy.deepcopy(self.lamb)\n",
    "        except:\n",
    "            temp_lambda = (self.lamb).clone()\n",
    "        a_array = []\n",
    "\n",
    "        # sample K-shot support data from the task\n",
    "        K_Xy = task.sample(self.K)\n",
    "        X = K_Xy[K_Xy.columns[-self.num_feature:]].copy().values\n",
    "        y = K_Xy[[\"y\"]].values\n",
    "        z = K_Xy[[\"z\"]].values\n",
    "        z_bar = np.mean(z) * np.ones((len(z), 1))\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float).unsqueeze(1)\n",
    "        y = torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
    "        ones = torch.tensor(np.ones((len(y), 1)), dtype=torch.float).unsqueeze(1)\n",
    "        z = torch.tensor(z, dtype=torch.float).unsqueeze(1)\n",
    "        z_bar = torch.tensor(z_bar, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        for co_update in range(self.pd_updates):\n",
    "            # inner_steps: number of steps of gradient, it is greater or equals to one in MAML\n",
    "            for step in range(self.inner_steps):\n",
    "                y_hat = self.model.parameterised(X, temp_weights)\n",
    "                fair = torch.abs(torch.mean((z - z_bar) * y_hat)) - self.c\n",
    "                loss = (-1.0) * torch.mean(y * torch.log(y_hat) + (ones - y) * torch.log(ones - y_hat)) + temp_lambda * fair\n",
    "                grad = torch.autograd.grad(loss.sum(), temp_weights)\n",
    "                temp_weights = [w - self.inner_lr * g for w, g in zip(temp_weights, grad)]\n",
    "            a_array.append(temp_weights)\n",
    "\n",
    "            tilde_weight = (*map(self.mean, zip(*a_array))),\n",
    "\n",
    "            gk = torch.abs(torch.mean((z - z_bar) * self.model.parameterised(X, tilde_weight))) - self.c\n",
    "            temp_lambda = temp_lambda + self.dual_ud_lr * gk\n",
    "            boolean = temp_lambda.item()\n",
    "            if boolean > 0:\n",
    "                temp_lambda = temp_lambda\n",
    "            else:\n",
    "                temp_lambda = 0\n",
    "\n",
    "        # sample new K-shot query data from the task for meta-update and compute loss\n",
    "        K_Xy = task.sample(self.Kq)\n",
    "        X = K_Xy[K_Xy.columns[-self.num_feature:]].copy().values\n",
    "        y = K_Xy[[\"y\"]].values\n",
    "        z = K_Xy[[\"z\"]].values\n",
    "        z_bar = np.mean(z) * np.ones((len(z), 1))\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float).unsqueeze(1)\n",
    "        y = torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
    "        z = torch.tensor(z, dtype=torch.float).unsqueeze(1)\n",
    "        z_bar = torch.tensor(z_bar, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        y_hat = self.model.parameterised(X, temp_weights)\n",
    "        fair = torch.abs(torch.mean((z - z_bar) * y_hat))\n",
    "        # print(\"y_hat\",y_hat)\n",
    "        # print('y',y)\n",
    "        # print('len y',len(y),len(y_hat))\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        y_hat = y_hat.detach().numpy().reshape(len(y_hat), 1)\n",
    "        y = y.detach().numpy().reshape(len(y), 1)\n",
    "        accuracy = accuracy_score(y_hat.round(), y)\n",
    "\n",
    "        return [loss, fair, accuracy]\n",
    "\n",
    "    def val_single_task(self, task):\n",
    "        # load trained parameters\n",
    "        temp_weights = [w.clone() for w in list(self.model.parameters())]\n",
    "        try:\n",
    "            temp_lambda = copy.deepcopy(self.lamb)\n",
    "        except:\n",
    "            temp_lambda = (self.lamb).clone()\n",
    "        a_array = []\n",
    "\n",
    "        # sample support data for testing\n",
    "        K_Xy = task.sample(self.K)\n",
    "        X = K_Xy[K_Xy.columns[-self.num_feature:]].copy().values\n",
    "        y = K_Xy[[\"y\"]].values\n",
    "        z = K_Xy[[\"z\"]].values\n",
    "        z_bar = np.mean(z) * np.ones((len(z), 1))\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float).unsqueeze(1)\n",
    "        y = torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
    "        ones = torch.tensor(np.ones((len(y), 1)), dtype=torch.float).unsqueeze(1)\n",
    "        z = torch.tensor(z, dtype=torch.float).unsqueeze(1)\n",
    "        z_bar = torch.tensor(z_bar, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        for co_update in range(self.pd_updates):\n",
    "            for step in range(self.inner_steps):\n",
    "                y_hat = self.model.parameterised(X, temp_weights)\n",
    "                fair = torch.abs(torch.mean((z - z_bar) * y_hat)) - self.c\n",
    "                loss = (-1.0) * torch.mean(y * torch.log(y_hat) + (ones - y) * torch.log(ones - y_hat)) + temp_lambda * fair\n",
    "                grad = torch.autograd.grad(loss.sum(), temp_weights)\n",
    "                temp_weights = [w - self.inner_lr * g for w, g in zip(temp_weights, grad)]\n",
    "            a_array.append(temp_weights)\n",
    "            tilde_weight = (*map(self.mean, zip(*a_array))),\n",
    "            gk = torch.abs(torch.mean((z - z_bar) * self.model.parameterised(X, tilde_weight))) - self.c\n",
    "            temp_lambda = temp_lambda + self.dual_ud_lr * gk\n",
    "            boolean = temp_lambda.item()\n",
    "            if boolean < 0:\n",
    "                temp_lambda = 0\n",
    "\n",
    "        # sample query data for testing\n",
    "        K_Xy = task.sample(self.Kq)\n",
    "        X = K_Xy[K_Xy.columns[-self.num_feature:]].copy().values\n",
    "        y = K_Xy[[\"y\"]].values\n",
    "        z = K_Xy[[\"z\"]].values\n",
    "        z_bar = np.mean(z) * np.ones((len(z), 1))\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float).unsqueeze(1)\n",
    "        y = torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
    "        z = torch.tensor(z, dtype=torch.float).unsqueeze(1)\n",
    "        z_bar = torch.tensor(z_bar, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        y_hat = self.model.parameterised(X, temp_weights)\n",
    "        fair = torch.abs(torch.mean((z - z_bar) * y_hat))\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        y_hat = y_hat.detach().numpy().reshape(len(y_hat), 1)\n",
    "        y = y.detach().numpy().reshape(len(y), 1)\n",
    "        accuracy = accuracy_score(y_hat.round(), y)\n",
    "\n",
    "        return [loss, fair, accuracy]\n",
    "\n",
    "    def val_tasks(self, tasks):\n",
    "        meta_loss = 0\n",
    "        meta_fair = 0\n",
    "        meta_accu = 0\n",
    "        for i in range(self.tasks_per_meta_batch):\n",
    "            task_name = random.choice(tasks)\n",
    "            val_task = pd.read_csv(self.val_tasks_path + '/' + task_name)\n",
    "            [t_loss, t_fair, t_accu] = self.val_single_task(val_task)\n",
    "            meta_loss += t_loss\n",
    "            meta_fair += t_fair\n",
    "            meta_accu += t_accu\n",
    "        avg_loss = meta_loss / self.tasks_per_meta_batch\n",
    "        avg_fair = meta_fair / self.tasks_per_meta_batch\n",
    "        avg_accu = meta_accu / self.tasks_per_meta_batch\n",
    "        return [avg_loss.item(), avg_fair.item(), avg_accu]\n",
    "\n",
    "    def main_loop(self, num_iterations, tasks_path):\n",
    "        train_loss = 0\n",
    "        train_fair = 0\n",
    "        train_accu = 0\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "            # compute meta loss\n",
    "            meta_loss = 0\n",
    "            meta_fair = 0\n",
    "            meta_accu = 0\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.tasks_per_meta_batch):\n",
    "                task_name = random.choice(self.tasks)\n",
    "                task = pd.read_csv(tasks_path + '/' + task_name)\n",
    "                [t_loss, t_fair, t_accu] = self.inner_loop(task)\n",
    "                meta_loss += t_loss\n",
    "                meta_fair += t_fair\n",
    "                meta_accu += t_accu\n",
    "\n",
    "            # compute meta gradient of loss with respect to maml weights\n",
    "            meta_grads = torch.autograd.grad(meta_loss, self.weights)\n",
    "\n",
    "            # assign meta gradient to weights and take optimisation step\n",
    "            for w, g in zip(self.weights, meta_grads):\n",
    "                w.grad = g\n",
    "            self.meta_optimiser.step()\n",
    "\n",
    "            # update meta dual variable\n",
    "            dual_update = self.lamb + self.meta_dual_lr * (meta_fair - self.tasks_per_meta_batch * self.c)\n",
    "            if dual_update.item() > 0:\n",
    "                self.lamb = dual_update\n",
    "            else:\n",
    "                self.lamb = 0\n",
    "\n",
    "            # log metrics\n",
    "            train_loss += meta_loss.item() / self.tasks_per_meta_batch\n",
    "            train_fair += meta_fair.item() / self.tasks_per_meta_batch\n",
    "            train_accu += meta_accu / self.tasks_per_meta_batch\n",
    "\n",
    "            self.train_losses_list.append(train_loss)\n",
    "            self.train_faires_list.append(train_fair)\n",
    "            self.train_accus_list.append(train_accu)\n",
    "\n",
    "            tasks_for_val = [f for f in listdir(self.val_tasks_path) if isfile(join(self.val_tasks_path, f))]\n",
    "            [val_loss, val_fair, val_accu] = self.val_tasks(tasks_for_val)\n",
    "\n",
    "            self.val_losses_list.append(val_loss)\n",
    "            self.val_faires_list.append(val_fair)\n",
    "            self.val_accus_list.append(val_accu)\n",
    "\n",
    "            if iteration % self.print_every == 0:\n",
    "                print(\"{}/{}. tr_loss: {}; tr_accu: {}; tr_fair: {}; val_loss: {}; val_accu: {}; val_fair: {} -----> running time: {} seconds.\".format(iteration, num_iterations, np.round(train_loss, 4),\n",
    "                                                                                                                      np.round(train_accu, 4),\n",
    "                                                                                                                      np.round(train_fair, 4), np.round(val_loss, 4), np.round(val_accu, 4),\n",
    "                                                                                                                      np.round(val_fair, 4), np.round((time.time() - start_time), 4)))\n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(train_loss / self.plot_every)\n",
    "                train_loss = 0\n",
    "                self.meta_faires.append(train_fair / self.plot_every)\n",
    "                train_fair = 0\n",
    "                self.meta_accus.append(train_accu / self.plot_every)\n",
    "                train_accu = 0\n",
    "\n",
    "        return [self.weights, self.lamb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bulgarian-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for training and testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "yellow-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a).to(dtype=torch.float) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fitting-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_task(train_model_save_path, task, meta_lamb):\n",
    "    # load the trained Fair MAML\n",
    "    net = MAMLModel()\n",
    "    net.load_state_dict(torch.load(train_model_save_path))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # load trained parameters\n",
    "    temp_weights = [w.clone() for w in list(net.parameters())]\n",
    "    try:\n",
    "        temp_lambda = copy.deepcopy(meta_lamb)\n",
    "    except:\n",
    "        temp_lambda = (meta_lamb).clone()\n",
    "    a_array = []\n",
    "\n",
    "    # sample support data for testing\n",
    "    K_Xy = task.sample(K)\n",
    "    X = K_Xy[K_Xy.columns[-num_feature:]].copy().values\n",
    "    y = K_Xy[[\"y\"]].values\n",
    "    z = K_Xy[[\"z\"]].values\n",
    "    z_bar = np.mean(z) * np.ones((len(z), 1))\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.float).unsqueeze(1)\n",
    "    y = torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
    "    ones = torch.tensor(np.ones((len(y), 1)), dtype=torch.float).unsqueeze(1)\n",
    "    z = torch.tensor(z, dtype=torch.float).unsqueeze(1)\n",
    "    z_bar = torch.tensor(z_bar, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    for co_update in range(pd_updates):\n",
    "        for step in range(inner_steps):\n",
    "            y_hat = net.parameterised(X, temp_weights)\n",
    "            fair = torch.abs(torch.mean((z - z_bar) * y_hat)) - c\n",
    "            loss = (-1.0) * torch.mean(y * torch.log(y_hat) + (ones - y) * torch.log(ones - y_hat)) + temp_lambda * fair\n",
    "            grad = torch.autograd.grad(loss.sum(), temp_weights)\n",
    "            temp_weights = [w - inner_lr * g for w, g in zip(temp_weights, grad)]\n",
    "        a_array.append(temp_weights)\n",
    "        tilde_weight = (*map(mean, zip(*a_array))),\n",
    "        gk = torch.abs(torch.mean((z - z_bar) * net.parameterised(X, tilde_weight))) - c\n",
    "        temp_lambda = temp_lambda + dual_ud_lr * gk\n",
    "        boolean = temp_lambda.item()\n",
    "        if boolean > 0:\n",
    "            temp_lambda = temp_lambda\n",
    "        else:\n",
    "            temp_lambda = 0\n",
    "\n",
    "\n",
    "    # sample query data for testing\n",
    "    K_Xy = task.sample(Kq)\n",
    "    X = K_Xy[K_Xy.columns[-num_feature:]].copy().values\n",
    "    y = K_Xy[[\"y\"]].values\n",
    "    z = K_Xy[[\"z\"]].values\n",
    "    X_temp = copy.deepcopy(X)\n",
    "    z_temp = copy.deepcopy(z)\n",
    "    z_bar = np.mean(z) * np.ones((len(z), 1))\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.float).unsqueeze(1)\n",
    "    y = torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
    "    z = torch.tensor(z, dtype=torch.float).unsqueeze(1)\n",
    "    z_bar = torch.tensor(z_bar, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    y_hat = net.parameterised(X, temp_weights)\n",
    "    fair = torch.abs(torch.mean((z - z_bar) * y_hat))\n",
    "\n",
    "    y_hat = y_hat.detach().numpy().reshape(len(y_hat), 1)\n",
    "    y = y.detach().numpy().reshape(len(y), 1)\n",
    "\n",
    "    input_zy = np.column_stack((z_temp, y_hat))\n",
    "    yX = np.column_stack((y_hat, X_temp))\n",
    "\n",
    "    accuracy = accuracy_score(y_hat.round(), y)\n",
    "    discrimination = cal_discrimination(input_zy)\n",
    "    consistency = cal_consistency(yX, num_neighbors)\n",
    "\n",
    "    return [accuracy, fair, discrimination, consistency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "embedded-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tasks(tasks, meta_lamb):\n",
    "    meta_accu = 0\n",
    "    meta_fair = 0\n",
    "    meta_disc = 0\n",
    "    meta_consis = 0\n",
    "    for i in range(tasks_per_meta_batch):\n",
    "        task_name = random.choice(tasks)\n",
    "        test_task = pd.read_csv(test_tasks_path + '/' + task_name)\n",
    "        [t_accu, t_fair, t_disc, t_consis] = test_single_task(train_model_save_path, test_task, meta_lamb)\n",
    "        meta_accu += t_accu\n",
    "        meta_fair += t_fair\n",
    "        meta_disc += t_disc\n",
    "        meta_consis += t_consis\n",
    "    avg_accu = meta_accu / tasks_per_meta_batch\n",
    "    avg_fair = meta_fair / (tasks_per_meta_batch)\n",
    "    avg_disc = meta_disc / (tasks_per_meta_batch)\n",
    "    avg_consis = meta_consis / (tasks_per_meta_batch)\n",
    "\n",
    "    return [avg_accu, avg_fair.item(), avg_disc, avg_consis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gross-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/30. tr_loss: 0.4653; tr_accu: 0.9688; tr_fair: 0.003; val_loss: 0.6979; val_accu: 0.5062; val_fair: 0.0027 -----> running time: 7.5426 seconds.\n",
      "---------------------------- 1 / 2 ------------------------------------------\n",
      "Accuracy for testing data = 0.50625\n",
      "DBC for testing data = 0.004715173505246639\n",
      "Discrimination for testing data = 0.020111261205229783\n",
      "Consistency for testing data = 0.9796088811010123\n",
      "-------------------------------------------------------------------------------\n",
      "25/30. tr_loss: 0.5681; tr_accu: 0.8; tr_fair: 0.0018; val_loss: 0.6969; val_accu: 0.5562; val_fair: 0.003 -----> running time: 7.8612 seconds.\n",
      "---------------------------- 2 / 2 ------------------------------------------\n",
      "Accuracy for testing data = 0.58125\n",
      "DBC for testing data = 0.0030298596248030663\n",
      "Discrimination for testing data = 0.014812198043384281\n",
      "Consistency for testing data = 0.9852548544295131\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# training and testing\n",
    "for i in range(repeat):\n",
    "        # -------------------------------------------  Training  ------------------------------------------#\n",
    "        # load training tasks\n",
    "        tasks_for_train = [f for f in listdir(train_tasks_path) if isfile(join(train_tasks_path, f))]\n",
    "\n",
    "        # train maml, output train loss, and plot meta losses\n",
    "        maml = MAML(MAMLModel(), tasks_for_train, num_feature, lamb, inner_lr, dual_ud_lr, meta_lr, meta_dual_lr, K, Kq, inner_steps, pd_updates, tasks_per_meta_batch, c, plot_every, print_every, val_tasks_path)\n",
    "        [meta_weights, meta_lamb] = maml.main_loop(num_iterations, train_tasks_path)\n",
    "        torch.save(maml.model.state_dict(), train_model_save_path)\n",
    "\n",
    "        # writing losses and faires into a file\n",
    "        with open(tr_val_meta_losses_faires_save_path, 'wb') as f:\n",
    "            pickle.dump(maml.train_losses_list, f)\n",
    "            pickle.dump(maml.train_faires_list, f)\n",
    "            pickle.dump(maml.val_losses_list, f)\n",
    "            pickle.dump(maml.val_faires_list, f)\n",
    "        ####################################################################################################\n",
    "\n",
    "        # load testing tasks\n",
    "        tasks_for_test = [f for f in listdir(test_tasks_path) if isfile(join(test_tasks_path, f))]\n",
    "\n",
    "        [test_accu, test_fair, test_disc, test_consis] = test_tasks(tasks_for_test, meta_lamb)\n",
    "        print('---------------------------- %s / %s ------------------------------------------' % (i + 1, repeat))\n",
    "        print('Accuracy for testing data =', test_accu)\n",
    "        print('DBC for testing data =', test_fair)\n",
    "        print('Discrimination for testing data =', test_disc)\n",
    "        print('Consistency for testing data =', test_consis)\n",
    "        print('-------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-vintage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
